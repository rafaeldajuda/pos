# -*- coding: utf-8 -*-
"""fase1-machine-learning-avancado-aula2-knn-svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wPc1y1r-Wijkw5wEbA0l37NdG8ceqr4s

# Recrutamento preditivo

A empresa de tecnologia HighTech busca contratar os melhores profissionais do mercado para fazer parte do time e gerar valor para a empresa. A HighTech vem investindo muito nos últimos anos no uso de ciência de dados no setor do RH para trazer ganhos no processo de seleção e recrutamento. O time de ciência de dados junto com o time de RH vem realizando juntos um projeto de recrutamento preditivo.

O recrutamento preditivo é uma técnica de People Analytics para encontrar os melhores candidatos para contratação da empresa, na prática, o recrutamento preditivo aumenta as chances do recrutador potencializar o processo de seleção. Por meio da coleta e análise de dados, é possível avaliar o perfil e o fit cultural dos profissionais para entender se existe uma boa aderência à vaga.

**Problema de negócio:**

O objetivo da HighTech é identificar quais são os melhores indicadores para realizar o recrutamento de profissionais.

**Base de dados**

Este conjunto de dados consiste em algumas características como: percentual de ensino médio e superior e especialização, experiência de trabalho e ofertas salariais para os profissionais colocados.

**Desafio**

Você como cientista de dados do time de dados da HighTech tem o desafio de criar um modelo preditivo de recrutamento para prever como e quais são as melhores variáveis que podem colocar um profissional bem qualificado na HighTech.
"""

import pandas as pd

dados = pd.read_excel('Recrutamento.xlsx')

dados.head(10)

dados.shape

set(dados.status)

dados.describe()

dados.info()

"""## Tratar dados nulos"""

# A biblioteca missingno é util para vizualização de dodos ausentes
import missingno as msno

msno.matrix(dados)

# isnull() retorna uma tabela mostrando quais linhas estão nulas ou não
dados.isnull()

# isnull().sum() retorna as colunas mais a informação de quantidades de linhas nulas
dados.isnull().sum()

import seaborn as sb

sb.boxplot(x='status', y='salary', data=dados, palette='hls')

# preencher dados nulos
# o parâmetro inplace indica se a função deve tratar os valores no próprio dataframe (True) ou retonar uma cópia com o tratamento (False)
dados['salary'].fillna(value=0, inplace=True)

dados.isnull().sum()

sb.boxplot(x=dados['hsc_p'])

"""*   A região azul é uma concentração de dados
*   A linha dentro da região azul é média dos dados
*   Os traços externos são a mínima é a máxima dos dados
*   Os pontos mais distantes são os dados discrepantes os outliers
"""

# sb.histplot(x=dados['hsc_p'])
sb.histplot(data=dados, x='hsc_p')

sb.boxplot(x=dados['degree_p'])

# sb.histplot(x=dados['degree_p'])
sb.histplot(data=dados, x='degree_p')

sb.boxplot(data=dados, x='etest_p')

sb.boxplot(data=dados, x='mba_p')

sb.boxplot(data=dados, x='salary')

sb.histplot(data=dados, x='salary')

"""Vamos ver se existe alguma relação entre o nível acadêmico e o status de contratação."""

sb.set_theme(style="whitegrid", palette="muted")
ax = sb.swarmplot(data=dados, x='mba_p', y='status', hue='workex')
ax.set(ylabel='mba_p')

! pip install plotly_express

import plotly_express as px

px.violin(dados, y='salary', x='specialisation', color='gender', box=True, points='all')

"""*   Nesse gráfico podemos observar que na especialização de Mk&RH possui uma diferença salarial entre gêneros, porém pequena
*   Já na especialização de Mk&Fin possui uma diferenã salarial mais expressiva entre homens e mulheres

## Correção entre variáveis com mapa de calor
"""

import matplotlib.pyplot as plt

correlation_matriz = dados.corr(numeric_only=True).round(2)

fig, ax = plt.subplots(figsize=(8, 8))

sb.heatmap(data=correlation_matriz, annot=True, linewidth=5, ax=ax)

"""*   Quanto mais clara a cor dos quadrados (ou maior o número) maior é a correlaçao entre as variáveis
*   Prestar atenção nas variáveis que se cruzam com ela mesmas, pois elas sempre vão ter uma forte correlção

## Pré-processamento de textos
"""

from sklearn.preprocessing import LabelEncoder

dados.head()

colunas = ['gender', 'workex', 'specialisation', 'status']

# Transformando textos em valores numéricos binários
label_encoder = LabelEncoder()
for col in colunas:
  dados[col] = label_encoder.fit_transform(dados[col])

dados.head()

# Nessa transformação cada valor da coluna se torna uma nova coluna
dummy_hsc_s = pd.get_dummies(data=dados['hsc_s'], prefix='dummy')
dummy_degree_t = pd.get_dummies(data=dados['degree_t'], prefix='dummy')

dados_dummy = pd.concat([dados, dummy_hsc_s, dummy_degree_t], axis=1)

dados_dummy.head(2)

dados_dummy.drop(['hsc_s', 'degree_t', 'salary'], axis=1, inplace=True)

dados_dummy.head()

correlation_matriz = dados_dummy.corr(numeric_only=True).round(2)

fig, ax = plt.subplots(figsize=(14, 14))

sb.heatmap(data=correlation_matriz, annot=True, linewidths=5, ax=ax)

"""# Construindo o modelo"""

x = dados_dummy[['ssc_p', 'hsc_p', 'degree_p', 'workex', 'mba_p']]
y = dados_dummy['status']

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=7)

x_train.shape

x_test.shape

x

"""Antes de construir o modelo é preciso padronizar (normalizar) as escalas dos valores."""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()

scaler.fit(x_train)

x_train_escalonado = scaler.transform(x_train)
x_test_escalonado = scaler.transform(x_test)

x_train_escalonado

"""## "Achando" o valor do k"""

import numpy as np

# Para descobrir o melhor valor de k iremos fazer vários testes de valores diferentes de k e ver qual tem o menor erro
error = []

for i in range (1, 10):
  knn = KNeighborsClassifier(n_neighbors=i)
  knn.fit(x_train_escalonado, y_train)
  pred_i = knn.predict(x_test_escalonado)

  error.append(np.mean(pred_i != y_test))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 10), error, color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10)
plt.title('Erro médio para K')
plt.xlabel('Valor de K')
plt.ylabel('Erro Médio')

"""*   Observando o gráfico os valores 5 e 6 possuem uma taxa de error menor comparados aos outros valores de k
*   Quando for escholer o valor de k é interessante escolher os números ímpares, pois com esse tipo de valor é possível desempatar caso os vizinhos próximos sejam números pares
"""

modelo_classificar = KNeighborsClassifier(n_neighbors=5)

modelo_classificar.fit(x_train_escalonado, y_train)

y_predito = modelo_classificar.predict(x_test_escalonado)

y_predito

# Checar a acurácia do modelo
from sklearn.metrics import accuracy_score

print(accuracy_score(y_test, y_predito))

"""## Modelo SVM - Support Vector Machine"""

from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline

svm = Pipeline([
    ('linear_svc', LinearSVC(C=1), ),
])

svm.fit(x_train_escalonado, y_train)

y_predito_svm = svm.predict(x_test_escalonado)

print(accuracy_score(y_test, y_predito_svm))

"""Comparando o KNN com o SVM, o KNN se saiu um pouco melhor. O SVM é sensível aos outliers, então talvez seja por isso que o modelo SVM se saiu um pouco pior comparado ao KNN.

É importante também observar os dados, se são estruturados e não estrututados, se são representativos ou não. A estrutura dos dados irão afetar cada modelos de forma diferente, então é sempre importante analisa-los.
"""