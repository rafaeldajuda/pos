# -*- coding: utf-8 -*-
"""fase1-aula2-machine-learning-regressao-linear-multipla.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eH2AZBjt_BzxtTCsWMjoG4xzW5cGtnQD

# Regressão Linear Multipla

A diferença entre a regressão linear simples da multipla é a quantidade de variávies. Enquanto a simples possui um coeficienete, a multipla possui mais de um coeficiente.

## Prever os valores médios das casas nos distritos da California

## Importando os dados

Antes de criar qualquer modelo é muito importante estudar os dados.
"""

import pandas as pd

dataset = pd.read_csv('housing.csv')

dataset.head()

"""## Configurações"""

# Commented out IPython magic to ensure Python compatibility.
# para deixar todas as saídas com os mesmos valores obtidos na live
import numpy as np # utilizado para calculos de vetores e matrizes
np.random.seed(42) # seed para manter sempre os mesmos dados de teste
import os

# Para plots bonitinhos
# permite a exibição de gráficos gerados pelo Matplotlib diretamente dentro do notebook ou ambiente de desenvolvimento, sem a necessidade de chamar explicitamente a função plt.show()
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

"""## Conhecendo os dados

"""

dataset.shape # mostrar a dimensão do dado (ex: 20640 linhas, 10 colunas)

dataset.info()

"""O **dataset.info()** traz informações sobre o volume de dados (20640) e informações sobre as colunas, por exemplo as tipagens. Uma observação interessante, a coluna **total_bedrooms** não possui a mesma quantidade de informações comparada as outras colunas, ou seja, dentro desta coluna existe valores nulos.

Uma outra observação, a variável **ocean_proximity** é do tipo object (string), podemos utilizar a **função value_counts()** para analisar quantas categorias existem e quantos bairros pertecem a essas categorias.
"""

set(dataset['ocean_proximity'])

dataset['ocean_proximity'].value_counts()

"""Podemos também analisar os dados do tipo númerico chamando a função **describe()**. Esta função ira trazer dados estatísticos de todas as variáveis do tipo numérico."""

dataset.describe()

"""Analisando algumas distribuições com histogramas:"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
dataset.hist(bins=50, figsize=(20,15))

"""Observações encontradas:


*   median_income (renda média) não parece estar expresso em dólares americanos, dê uma olhada na escala dos dados. Alguns dados podem vir com tratamentos ou outras dimensões não tão comuns.
*   a variável median_house_value (nossa target) também parece estar limitada em casas até 500 mil dólares. Aqui seria interessante entender com o cliente se o mesmo deseja que o algoritmo ultrapasse esse limite ou não.
*   podemos identificar bastantes assimetria nos dados.

## Separando as bases em treino e teste
"""

from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(dataset, test_size=0.2, random_state=7)

print(len(df_train), "treinamento +", len(df_test), "teste")

"""## Criando categorias de média de randa anual

Vamos supor que algum especialista de área de vendas de imóveis notificou que a **média de renda anual é um atributo importante para colocar no modelo preditivo praa estimar preços médios**

Quando dividimos o conjunto de treino e teste **precisamos garantir que ambos sejam representativos com todos os valores de renda anual**. Como a média de renda anual é um atributo numérico, que tal criar uma categoria de renda?

"""

dataset['median_income'].hist()

# Dividindo por 1,5 para limitar o número de categorias de renda
# dividendo o valor da coluna "median_income" de cada entrada pelo valor 1,5 e, em seguida, arrendondando o resultado para cima usando a função
# np.ceil() (da biblioteca Numpy). Isso cria uma nova coluna chamada "income_cat" no dataset que contém os valores das categorias de renda após
# a divisão e arredondamento.

dataset['income_cat'] = np.ceil(dataset['median_income'] / 1.5) # ceil para arredondar os valores para cima

# Rotulo aqueles acima de 5 como 5.
# os valores na coluna "income_cat" que forem maiores ou iguais a 5 são substituídos por 5. Isso é feito usando a função .where() do pandas.
# Basicamente, se o valor em "income_cat" for menor que 5, ele permance o mesmo; caso o contrário, é substituído por 5.

dataset['income_cat'].where(dataset['income_cat']  < 5, 5.0, inplace=True)

# cut do Pandas, que é comunente usada para dividir um conjunto de dados em intervalos discretos chamados de "bins" (intervalos ou faixas)
dataset['income_cat'] = pd.cut(dataset['median_income'],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])

dataset['income_cat'].value_counts()

dataset['income_cat'].hist()

"""Resumindo, esse código está transformando valores contínuos de renda em categorias discretas, dividindo-os em intervalos específicos e  arredondando-os para cima, garantindo que o número de categorias seja limitado e, finalmente, atribuindo rótulos numéricos a essas categorias.

Agora com as categorias criadas, vamos realizar a amostragem estratificada com base na categoria de renda.
"""

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(dataset, dataset['income_cat']):
  strat_train_set = dataset.loc[train_index]
  strat_test_set = dataset.loc[test_index]

# Analisando as proporções
strat_test_set['income_cat'].value_counts() / len(strat_test_set)

# Analisando as proporções
strat_train_set['income_cat'].value_counts() / len(strat_train_set)

dataset['income_cat'].value_counts() / len(dataset)

"""Depois de garantir que os valores médios de renda anual estão distribuídos de forma estratificada, podemos remover a coluna income_cat que utilizamos como varável auxilixar."""

# Removendo o income_cat das bases de treino e teste
# O uso do termo set_ é uma convesão para indicar que é uma variável temporária que itera sobre um conjunto de dados (um conjunto de treinamento ou um conjunto de teste).

for set_ in (strat_train_set, strat_test_set):
  set_.drop('income_cat', axis=1, inplace=True)

"""## Analisando gráficos geográficos"""

housing = strat_train_set.copy()

housing.plot(kind='scatter',  x='longitude', y='latitude') # scatter é im gráfico de dispersão

"""Uma observação, a distribuição dos pontos se assemelha ao estado da California."""

housing.plot(kind='scatter',  x='longitude', y='latitude', alpha=0.1)

"""O parâmetro **alpha** destaca os pontos onde possuem maior concentração.

## Analisando preços imobiliários
"""

housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4,
             s=housing['population'] / 100, label='population', figsize=(10,7),
             c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True,
             sharex=False)
plt.legend()

"""Podemos identificar que os preços estão muitos relacionados com a localização (por exemplo, próximo do oceano) e com densidade populacional.

## Buscando correlações
"""

housing.head()

corr_matrix = housing.corr(numeric_only=True)

corr_matrix['median_house_value'].sort_values(ascending=False)

from pandas.plotting import scatter_matrix

attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']
scatter_matrix(housing[attributes], figsize=(12, 8))

"""Analisando as correlações, a feature que seja mais promissora para prever o valor médio da habitação é a **renda média**.

median_income: 0.68

Vamos plotar essas duas features em um gráfico de scatter para analisar com mais detalhe.
"""

housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)
plt.axis([0, 16, 0, 550000])

"""Observamos que: A correlação realmente mostra uma certa tendência ascendente nos dados e os pontos não estão mais dispersos.

O limite de preço que temos na base de dados é claramente visível como uma linha horizontal em 500 mil doláres. Observe também que temos essas linhas retas (claro que menos óbvias) na horizotal em torno de 450 mil, outra em 350 mil dólares e uma em 280 mil dólares.

Será que deixamos essas peculariedades nos dados, não pode afetar nosso algoritmo?

## Preparando os dados para colocar no algoritmo
"""

housing = strat_train_set.drop('median_house_value', axis=1) # apagando a target para a base de treino (nosso x)
housing_labels = strat_train_set['median_house_value'].copy() # armazenando a target (nosso y)

# listando as colunas nulas

sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()
sample_incomplete_rows

housing.isnull().sum()

"""Como vamos tratar esses valores nulos?"""

# Opção 1
# Substituindo os valres nulos pela mediana
median = housing['total_bedrooms'].median()
sample_incomplete_rows['total_bedrooms'].fillna(median, inplace=True)
sample_incomplete_rows

"""## Utilizando as classes do Sklearn

Você também pode optar por utilizar classes acessíveis do Sklearn.

Você pode criar pipelines de pré-processamento e modelagem com facilidade usando as classes do Scikit-Learn. Isso permite criar fluxos de trabalho mais organizados e repetíveis.

Vamos utilizar o **Imputer** para substituir os valores faltantes pela média.
"""

# opção 2
try:
  from sklearn.impute import SimpleImputer # Scikit-Learn 0.20+
except ImportError:
  from sklearn.preprocessing import Imputer as SimpleImputer

imputer = SimpleImputer(strategy='median')

"""Remova o atributo de texto porque a mediana só pode ser calculada em atributos numéricos:"""

housing_num = housing.drop('ocean_proximity', axis=1)

imputer.fit(housing_num) # calculando a mediana de cada atributo e armazenando o resultado na variável statistics_

imputer.statistics_

housing_num.median().values

"""Aplicando o Imputer "treinado" na base para substituir valores faltantes perdidos pela mediana:"""

X = imputer.transform(housing_num) # resultado é um array

X

housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing.index)

housing_tr

# verificando os resultados
housing_tr.loc[sample_incomplete_rows.index.values]

imputer.strategy

housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)
housing_tr.head()

"""## Pré-processando as categorias

Agora vamos pré-pricossar o recurso de entrada categórica, `ocean_proximity`.
"""

housing_cat = housing[['ocean_proximity']]
housing_cat.head(10)

"""O OrdinalEncoder é uma classe da biblioteca scikit-learn, usada para transformar variáveis categoricas ordinais em valores númericos. Variáveis ordinais são aquelas que tem uma ordem ou hierarquia específica, mas as distâncias entre valores não são necessariamente signicativas."""

try:
  from sklearn.preprocessing import OrdinalEncoder
except:
  from future_enconders import OrdinalEncoder # Scikit-Learn < 0.20

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]

ordinal_encoder.categories_

"""O OneHotEncoder é outra classe da biblioteca scikit-learn, usada para transformar variáveis categóricas em representações numéricas binárias. Ele é partilarmente útil quando se lida com variáveis categóricas nominais, ou seja, aquelas que não têm uma ordem específica."""

try:
  from sklearn.preprocessing import OrdinalEncoder # apenas para gerar um ImportError se Scikit-Learn < 0.20
  from sklearn.preprocessing import OneHotEncoder
except:
  from future_encoders import OneHotEncoder # Scikit-Learn < 0.20

cat_encoder = OneHotEncoder(sparse=False)
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot

cat_encoder.categories_

"""## Criando a pipeline de pré-processamento dos dados

Agora vamos construir uma pipeline para pré-processar os atributos numéricos
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler # funções que retorna a média e o desvio padrão dos dados

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")), #substituindo valores nulos pela mediana
        ('std_scaler', StandardScaler()), # padronizando as escalas dos dados
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)

housing_num_tr

"""Agora vamos tratar os valores categóricos

O **Column Transformer** é uma classe da biblioteca scikit-learn em Python que permite aplicar transformações específicas a diferentes colunas de um conjunto de dados (dados numéricos, categóricos, etc). e deseja aplicar diferentes pré-processamentos ou transformações a cada tipo de coluna.
"""

try:
  from sklearn.compose import ColumnTransformer
except ImportError:
  from future_encoders import ColumnTransformer # Scikit-Learn < 0.20

from sklearn.compose import ColumnTransformer

num_attribs = list(housing_num)
cat_attribs = ['ocean_proximity']

full_pipeline = ColumnTransformer([
    ('num', num_pipeline, num_attribs), # tratrando as variáveis numéricas (chamada a pipeline de cima)
    ('cat', OneHotEncoder(), cat_attribs), # tratrando as variáveis categóricas
])

housing_prepared = full_pipeline.fit_transform(housing)

housing_prepared

housing_prepared.shape

type(housing_prepared)

"""Perceba que o resultado é uma matriz multidimensional. Precisamos transformá-la em dataframe."""

column_names = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms',
    'population', 'households', 'median_income', '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']

# Transformar o array em DataFrame
housing_df = pd.DataFrame(data=housing_prepared, columns=column_names)

# Exibir o DataFrame resultante
print(housing_df.shape)

housing_df.head()

print(housing_df.isnull().sum())

"""Agora os nossos dados estão limpos e organizados.

## Escolhendo o melhor modelo de regressão

Vamos começar com a velha e boa regressão linear!

- Equação do 1° grau.

- A Regressão Linear busca entender o padrão de um valor dependendo de outro ou outros, e assim encontrar uma função que expressa esse padrão.

- **Foco**: buscar o melhor valor que os coeficientes possam atingir, de maneira que a diferença entre o valor predito pela função e o real, sejam os menores.
"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)

# vamos tentar o pipeline de pré-processamento completo em algumas instâncias de treinamento
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]

some_data_prepared = full_pipeline.transform(some_data)

predictions = lin_reg.predict(housing_prepared)

print('Predictions:', lin_reg.predict(some_data_prepared))

"""Compare com os valores reais:"""

print('Labels:', list(some_labels))

"""## Avaliando o modelo

O **MSE** mede a média dos quadrados das diferenças entre os valores previstos pelo modelo e os valores reais observados no conjunto de dados.

Quanto menor o valor do MSE, melhor o ajuste do modelo aos dados.
"""

from sklearn.metrics import mean_squared_error
# erro médio quadrático eleva ao quadrado a média do erro médio absoluto. Estou avaliando se os erros não são tão grandes, esses erros são penalizados.
# penaliza muito mais valores distantes da média

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse) # raiz quadrada aqui
lin_rmse

from sklearn.metrics import mean_absolute_error

lin_mae = mean_absolute_error(housing_labels, housing_predictions)
lin_mae

"""Um erro de margem de 69050 dólares não é muito aceitável no nosso modelo sendo que os valores de median_housing_values variam entre  120 mil dólares e 265 mil dólares. Podemos definir aqui que esse modelo está com overfiting.
Vamos tentar um modelo mais poderoso?
"""

from sklearn.metrics import r2_score

r2 = r2_score(housing_labels, housing_predictions)
print('r²', r2)

# Função para calcular o MAPE (Mean Absolute Percentage Error)

def calculate_mape(labels, predictions):
  errors = np.abs(labels - predictions)
  relative_errors = errors / np.abs(labels)
  mape = np.mean(relative_errors) * 100
  return mape

# Calcular MAPE
mape_result = calculate_mape(housing_labels, housing_predictions)

# Imprimir o resultado
print(f'O MAPE É: {mape_result:.2f}%')

"""## Que tal tentar outros models"""

from sklearn.tree import DecisionTreeRegressor

# Criando o modelo de DecisionTreeRegressor
model_dtr = DecisionTreeRegressor(max_depth=10)
model_dtr.fit(housing_prepared, housing_labels)

# vamos tentar o pipeline de pré-processamento completo em algumas instâncias de treinamento
some_data = housing.iloc[:5]
some_labels = housing.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)
predictions = model_dtr.predict(some_data_prepared)

print('Predictions:', model_dtr.predict(some_data_prepared))

print('Labels:', list(some_labels))

# mean_squared_error
housing_predictions = model_dtr.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

# mean_absolute_error
lin_mae = mean_absolute_error(housing_labels, housing_predictions)
lin_mae

r2 = r2_score(housing_labels, housing_predictions)
print('r²', r2)

# Calcular o MAPE
mape_result = calculate_mape(housing_labels, housing_predictions)

# Imprimir o resultado
print(f'O MAPE é: {mape_result:.2f}%')