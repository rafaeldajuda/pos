# -*- coding: utf-8 -*-
"""fase1_pln_aula5_word_embedings_com_word2vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlNKNcYS4Nq6hsQuZXi5c7-Vcle_ZamQ

# Word embedings com word2vec

Word embeddings são representações vetoriais de palavras em um espaço dimensional, onde palavras semanticamente semelhantes estão próximas umas das outras. Essas representações são aprendidas por modelos de linguagem a partir de grandes quantidades de texto não rotulado. O conceito subjacente é que palavras com significados semelhantes ou que ocorrem em contextos semelhantes no texto devem ter representações vetoriais similares.

##Principais características dos word embeddings:

1. Densidade: Ao contrário de outras representações de palavras, como one-hot encoding, que resultam em vetores esparsos de alta dimensionalidade, word embeddings são densos, o que significa que cada palavra é representada por um vetor de números reais de tamanho fixo.

2. Contextualização: Os word embeddings capturam o significado contextual das palavras, ou seja, uma palavra pode ter diferentes representações dependendo do contexto em que ocorre.

3. Similaridade semântica: Palavras semanticamente relacionadas tendem a ter representações vetoriais próximas umas das outras. Por exemplo, os vetores de "cachorro" e "gato" estarão mais próximos no espaço vetorial do que os vetores de "cachorro" e "banana".

4. Transferência de aprendizado: Word embeddings pré-treinados podem ser usados como recursos em tarefas de processamento de linguagem natural (PLN), como classificação de texto, análise de sentimento, tradução automática, entre outros. Essa técnica de transferência de aprendizado permite melhorar o desempenho de modelos de PLN, especialmente em conjuntos de dados com poucas amostras.

5. Redução de dimensionalidade: As representações vetoriais de palavras permitem uma redução de dimensionalidade, o que facilita a manipulação e o processamento de grandes conjuntos de dados textuais.

Alguns dos algoritmos populares para aprender word embeddings incluem Word2Vec, GloVe (Global Vectors for Word Representation) e FastText. Esses modelos são treinados em grandes corpora de texto e capturam informações semânticas e sintáticas das palavras. Os word embeddings desempenham um papel fundamental em muitas aplicações de processamento de linguagem natural e têm sido uma área ativa de pesquisa nos últimos anos.
"""

# fonte dos arquivos
# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc

!unzip "/content/drive/MyDrive/Colab Notebooks/Word2Vec/cbow_s300.zip"

import pandas as pd

# os dados podem ser baixados na plataforma fiap
artigo_treino = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Word2Vec/treino.csv")
artigo_teste = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Word2Vec/teste.csv")

"""# Objetivo

Os dados são notícias de vários sites, o nosso objetivo é criar um classificador com os dados das notícias.

"""

artigo_treino.head()

artigo_teste.head()

print(artigo_treino.shape)
print(artigo_teste.shape)

"""# One Hot Enconding"""

from sklearn.feature_extraction.text import CountVectorizer

texto = ['Esse produto é muito bom', 'Esse produto é muito ruim horrível']

vetorizador = CountVectorizer()
vetorizador.fit(texto)

print(vetorizador.vocabulary_)

"""**OBS:**  O CountVectorizer aplica o conceito de stop words, então nesse caso a palavra 'é' não foi considerada, pois ela é uma stop word."""

# posicoes do vocabulario
# 0 0 0 1 0 0 -> muito
vetor_muito = vetorizador.transform(["muito"])
print(vetor_muito.toarray())

"""# CBOW vs Skip-gram

CBOW (Continuous Bag of Words) e Skip-gram são duas arquiteturas de rede neural utilizadas no treinamento de word embeddings, popularizadas pelo modelo Word2Vec, desenvolvido por Tomas Mikolov e sua equipe no Google.

## Continuous Bag of Words (CBOW)

1. **Objetivo**: O objetivo do CBOW é prever a palavra central (target word) dada uma janela de palavras de contexto (context window).

2. **Entrada e saída**:
   - **Entrada**: Um conjunto de palavras de contexto ao redor da palavra central.
   - **Saída**: A palavra central que está sendo prevista.

3. **Como funciona**:
   - A rede neural CBOW recebe as palavras de contexto como entrada e tenta prever a palavra central que está no meio dessas palavras.
   - Por exemplo, para a frase "o gato está no tapete", se a palavra central for "está", as palavras de contexto seriam ["o", "gato", "no", "tapete"].
   - O modelo usa essas palavras de contexto para prever "está".

4. **Vantagem**:
   - CBOW tende a ser mais rápido e eficiente porque utiliza múltiplas palavras de contexto para prever uma única palavra, agregando informações das palavras de contexto em uma única predição.

## Skip-gram

1. **Objetivo**: O objetivo do Skip-gram é prever as palavras de contexto dada uma palavra central.

2. **Entrada e saída**:
   - **Entrada**: A palavra central.
   - **Saída**: As palavras de contexto ao redor da palavra central.

3. **Como funciona**:
   - A rede neural Skip-gram recebe uma palavra central como entrada e tenta prever as palavras de contexto ao redor dela.
   - Por exemplo, para a mesma frase "o gato está no tapete", se a palavra central for "está", o modelo tentará prever as palavras de contexto ["o", "gato", "no", "tapete"].

4. **Vantagem**:
   - Skip-gram é eficaz para treinar bons embeddings mesmo em datasets menores e é capaz de capturar melhor as relações semânticas e sintáticas entre palavras, especialmente para palavras raras.

## Resumo das diferenças

- **Direção da predição**:
  - **CBOW**: Prevê a palavra central a partir das palavras de contexto.
  - **Skip-gram**: Prevê as palavras de contexto a partir da palavra central.

- **Eficiência**:
  - **CBOW**: Geralmente mais rápido, adequado para grandes corpora.
  - **Skip-gram**: Pode ser mais lento, mas captura melhor as relações para palavras raras.

- **Complexidade**:
  - **CBOW**: Usa uma média das representações das palavras de contexto, resultando em uma rede neural mais simples.
  - **Skip-gram**: Treina diretamente em pares de palavras central-contexto, gerando mais combinações e uma rede neural potencialmente mais complexa.

Ambos os métodos têm suas próprias vantagens e desvantagens, e a escolha entre eles pode depender do tipo de dados e do objetivo específico da aplicação de processamento de linguagem natural.

# CBOW
"""

with open('cbow_s300.txt') as f:
  for linha in range(50):
    print(next(f))

# gensim - https://radimrehurek.com/gensim/models/keyedvectors.html
from gensim.models import KeyedVectors

modelo = KeyedVectors.load_word2vec_format('cbow_s300.txt')

modelo.get_vector("china")

modelo.most_similar("china")

modelo.most_similar("brasil")

modelo.most_similar(positive=["brasil","uruguai"])

modelo.most_similar(positive=["china","russia"])

"""**OBS:** Fazendo uma breve observação sobre a relação positiva dos dados entre a China e a Rússia, podemos dizer os países mostrados poderiam ser um local para negócios dependendo do objetivo de alguma empresa."""

# checando se é possível checar palavras no plural
# carros + foguete - carro = foguetes

modelo.most_similar(positive=['carros', 'foguete'], negative=['carro'])

"""# Classificador CBOW"""

import nltk
import string
nltk.download('punkt')

def tokenizador(texto):
  texto = texto.lower()
  lista_alfanumerico = []

  for token_valido in nltk.word_tokenize(texto):
    if token_valido in string.punctuation: continue
    lista_alfanumerico.append(token_valido)

  return lista_alfanumerico

tokenizador('Texto com, pontuação')

import numpy as np

def combinacao_de_vetores_por_soma(palavras_numeros):
  vetor_resultante = np.zeros(300)
  for pn in palavras_numeros:
    try:
      vetor_resultante =+ modelo.get_vector(pn)
    except KeyError:
      if pn.isnumeric():
        pn = '0'*len(pn)
        vetor_resultante =+ modelo.get_vector(pn)
      else:
        vetor_resultante =+ modelo.get_vector('unknown')
  return vetor_resultante

palavras_numeros = tokenizador('Texto fiaps')
vetor_texto = combinacao_de_vetores_por_soma(palavras_numeros)
print(len(vetor_texto))
print(vetor_texto)

def matriz_vetores(textos):
  x = len(textos)
  y = 300
  matriz = np.zeros((x, y))

  for i in range(x):
    palavras_numeros = tokenizador(textos.iloc[i])
    matriz[i] = combinacao_de_vetores_por_soma(palavras_numeros)

  return matriz

matriz_vetores_treino = matriz_vetores(artigo_treino.title)
matriz_vetores_teste = matriz_vetores(artigo_teste.title)
print(matriz_vetores_treino.shape)
print(matriz_vetores_teste.shape)

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression()
LR.fit(matriz_vetores_treino, artigo_treino.category)

# avalição o modelo em cima de todas categorias
LR.score(matriz_vetores_teste, artigo_teste.category)

artigo_teste.category.unique()

# avaliando o modelo por categoria
from sklearn.metrics import classification_report

label_prevista = LR.predict(matriz_vetores_teste)
CR = classification_report(artigo_teste.category, label_prevista)
print(CR)

from sklearn.dummy import DummyClassifier

DC = DummyClassifier()
DC.fit(matriz_vetores_treino, artigo_treino.category)
label_prevista_dc = DC.predict(matriz_vetores_teste)
CR_dummy = classification_report(artigo_teste.category, label_prevista_dc)
print(CR_dummy)

"""# Classificador Skip Gram"""

# fonte dos arquivos
# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc

!unzip "/content/drive/MyDrive/Colab Notebooks/Word2Vec/skip_s300.zip"

modelo_skipgram = KeyedVectors.load_word2vec_format("skip_s300.txt")

def combinacao_de_vetores_por_soma_skipgram(palavras_numeros):
  vetor_resultante = np.zeros(300)
  for pn in palavras_numeros:
    try:
      vetor_resultante =+ modelo_skipgram.get_vector(pn)
    except KeyError:
      if pn.isnumeric():
        pn = '0'*len(pn)
        vetor_resultante =+ modelo_skipgram.get_vector(pn)
      else:
        vetor_resultante =+ modelo_skipgram.get_vector('unknown')
  return vetor_resultante

def matriz_vetores_skipgram(textos):
  x = len(textos)
  y = 300
  matriz = np.zeros((x, y))

  for i in range(x):
    palavras_numeros = tokenizador(textos.iloc[i])
    matriz[i] = combinacao_de_vetores_por_soma_skipgram(palavras_numeros)

  return matriz

matriz_vetores_treino_skipgram = matriz_vetores_skipgram(artigo_treino.title)
matriz_vetores_teste_skipgram = matriz_vetores_skipgram(artigo_teste.title)

LR_skipgram = LogisticRegression(max_iter=200)
LR_skipgram.fit(matriz_vetores_treino_skipgram, artigo_treino.category)
label_previsao_skipgram = LR_skipgram.predict(matriz_vetores_teste_skipgram)
CR_skipgram = classification_report(artigo_teste.category, label_previsao_skipgram)
print(CR_skipgram)

print(CR)