# -*- coding: utf-8 -*-
"""fase1_pln_aula4_stemming_tf_idf_ngrams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9R_1p_JsEUHL_K0gR2U8GmllYSrKfYW

# Criando um modelo classificar e avaliação

Utilizando uma base de dados do busca de pé iremos criar um modelo para classificar e avaliar os comentários.

## Importando a base de dados

https://www.kaggle.com/code/abnerfreitas/nlp-buscape-data-ptbr-sentiment-analysis/input
"""

import pandas as pd

avaliacoes = pd.read_csv('b2w.csv')

avaliacoes.head()

"""## Limpeza da base de dados

Retirando as colunas que não são necessárias.
"""

avaliacoes = avaliacoes.drop(['original_index', 'review_text_processed', 'review_text_tokenized', 'rating', 'kfold_polarity', 'kfold_rating'], axis=1)

avaliacoes.head()

"""Remevendo as colunas com valores nulos."""

avaliacoes.info()

avaliacoes.isnull().all().count().sum()

avaliacoes.dropna(inplace=True, axis=0)
avaliacoes

# avaliacoes.polarity.value_counts()
avaliacoes['polarity'].value_counts()

"""## Separando os dados de treino e teste"""

from sklearn.model_selection import train_test_split

treino, teste, classe_treino, classe_teste = train_test_split(avaliacoes.review_text, avaliacoes.polarity,
                                                              stratify=avaliacoes.polarity,
                                                              random_state=71)

classe_teste

"""## Criando um modelo"""

# Linear Model - Logistic Regression
# from sklearn.linear_model import LogisticRegression

# regressao_ligistica = LogisticRegression()
# regressao_ligistica.fit(treino, classe_treino)
# acuracia = regressao_ligistica.score(teste, classe_teste)
# print(acuracia)

# bag of words
from sklearn.feature_extraction.text import CountVectorizer

texto = ['Este produto é muito bom', 'Este produto é muito ruim']
vetorizar = CountVectorizer()
bag_of_words = vetorizar.fit_transform(texto)

bag_of_words

matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names_out())

matriz_esparsa

"""Vamos aplicar o bag of words na nossa base"""

vetorizar = CountVectorizer()
bag_of_words = vetorizar.fit_transform(avaliacoes.review_text)
print(bag_of_words.shape)

"""116058 -> quatidade de linhas<br/>
50508 -> quantidade de palavras únicas
"""

vetorizar = CountVectorizer(max_features=100) # max_features -> limitando para as 100 palavras que mais se repetem
bag_of_words = vetorizar.fit_transform(avaliacoes.review_text)
print(bag_of_words.shape)

from sklearn.linear_model import LogisticRegression

treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                              avaliacoes.polarity,
                                                              stratify=avaliacoes.polarity,
                                                              random_state=71)

regressao_logistica = LogisticRegression()
regressao_logistica.fit(treino, classe_treino)
acuracia = regressao_logistica.score(teste, classe_teste)
print(acuracia)

from sklearn.linear_model import LogisticRegression

def treinar_modelo(dados, coluna_texto, coluna_sentimento):
  vetorizar = CountVectorizer(max_features=100) # max_features -> limitando para as 100 palavras que mais se repetem
  bag_of_words = vetorizar.fit_transform(avaliacoes[coluna_texto])

  treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                              avaliacoes[coluna_sentimento],
                                                              stratify=avaliacoes[coluna_sentimento],
                                                              random_state=71)

  regressao_logistica = LogisticRegression()
  regressao_logistica.fit(treino, classe_treino)
  return regressao_logistica.score(teste, classe_teste)

print(treinar_modelo(avaliacoes, 'review_text', 'polarity'))

"""# World Cloud

Visualizando as principais reviews com uma word cloud.

https://github.com/amueller/word_cloud
"""

from wordcloud import WordCloud

todas_avaliacoes = [texto for texto in avaliacoes.review_text]
todas_palavras = ' '.join(todas_avaliacoes)

len(todas_avaliacoes)

len(todas_palavras)

nuvem_palavras = WordCloud().generate(todas_palavras)

nuvem_palavras

import matplotlib.pyplot as plt

plt.figure()
plt.imshow(nuvem_palavras)

nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110).generate(todas_palavras)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))
plt.imshow(nuvem_palavras, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Avaliações negativas e positivas"""

def word_cloud_neg(dados, coluna_texto):
  texto_negativo = dados.query('polarity == 0')
  todas_avaliacoes = [texto for texto in texto_negativo[coluna_texto]]
  todas_palavras = ' '.join(todas_avaliacoes)
  # collocations -> retira as palvras em conjunto (ex: muito bom) e deixa somente palavras únicas
  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10, 7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis('off')
  plt.show()

def word_cloud_pos(dados, coluna_texto):
  texto_positivo = dados.query('polarity == 1')
  todas_avaliacoes = [texto for texto in texto_positivo[coluna_texto]]
  todas_palavras = ' '.join(todas_avaliacoes)
  # collocations -> retira as palvras em conjunto (ex: muito bom) e deixa somente palavras únicas
  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10, 7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis('off')
  plt.show()

word_cloud_neg(avaliacoes, 'review_text')

word_cloud_pos(avaliacoes, 'review_text')

"""OBS: nos gráficos existem as **stop words**, que são palavras que não fazem muito sentido na avalição do gráfico. Ex: na, dos, ao, já, ...

# NLTK

(chatgpt)<br/>
A biblioteca NLTK (Natural Language Toolkit) é uma das bibliotecas mais populares em Python para processamento de linguagem natural. Ela oferece uma variedade de ferramentas para trabalhar com textos em várias línguas, com aplicações em áreas como análise de sentimentos, extração de informações e análise semântica.

Recursos principais da NLTK:

* Tokenização: Dividir um texto em unidades linguísticas, como palavras ou sentenças.
* Stemming e lematização: Reduzir palavras à sua forma base (radical) ou forma léxica (lemma).
* Remoção de stopwords: Remover palavras comuns (como artigos e preposições) que não contribuem para o significado do texto.
* Part-of-Speech Tagging: Atribuir etiquetas gramaticais a cada palavra de um texto.
* Chunking: Identificar frases ou trechos de texto com base em suas partes do discurso.
* Classificação e análise de sentimentos: Ferramentas para classificar textos ou analisar sentimentos expressos em textos.
* Corpora e léxicos: Conjuntos de textos anotados e dicionários para ajudar nas tarefas de processamento de linguagem natural.

A NLTK é frequentemente usada em conjunto com outras bibliotecas de aprendizado de máquina para construir modelos mais complexos. É uma biblioteca de grande importância para pesquisadores e profissionais que trabalham com textos e análise de dados textuais.
"""

# https://www.nltk.org/

import nltk
nltk.download("all") # baixa todos pacotes de dados do nltk

# Contando a frequência das frases

corpus = ["Muito este produto", "Muito ruim este produto"]
frequencia = nltk.FreqDist(corpus)
frequencia

from nltk import tokenize

frase = "Muito bom este produto"

token_por_espaco = tokenize.WhitespaceTokenizer()
token_frase = token_por_espaco.tokenize(frase) # ira gerar um array separando as strings pelo espaço
token_frase

token_por_espaco = tokenize.WhitespaceTokenizer()
token_dataset = token_por_espaco.tokenize(todas_palavras)
frequencia = nltk.FreqDist(token_dataset)

frequencia

dataframe_frequencia = pd.DataFrame({'palavra': list(frequencia.keys()), 'frequencia': list(frequencia.values())})

dataframe_frequencia.head()

# listar as palavras mais frequêntes
dataframe_frequencia.nlargest(columns='frequencia', n=10)

import seaborn as sns

plt.figure(figsize=(12, 8))
ax = sns.barplot(data = dataframe_frequencia.nlargest(columns='frequencia', n=10), x='palavra', y='frequencia', color='lightblue')
ax.set(ylabel='Contagem')
plt.show()

def grafico(dados, coluna_texto, quantidade):
  todas_palavras = ' '.join(texto for texto in dados[coluna_texto])
  token_frase = token_por_espaco.tokenize(todas_palavras)
  frequencia = nltk.FreqDist(token_frase)
  dataframe_frequencia = pd.DataFrame({'palavra': list(frequencia.keys()), 'frequencia': list(frequencia.values())})
  dataframe_frequencia = dataframe_frequencia.nlargest(columns='frequencia', n=quantidade)

  plt.figure(figsize=(12, 8))
  ax = sns.barplot(data = dataframe_frequencia, x='palavra', y='frequencia', color='lightblue')
  ax.set(ylabel='Contagem')
  plt.show()

grafico(avaliacoes, 'review_text', 20)

"""# Remoção de Stop Words"""

palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')
palavras_irrelevantes

frase_processada = list()
for avaliacao in avaliacoes.review_text:
  nova_frase = list()
  palavras_texto  = token_por_espaco.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in palavras_irrelevantes:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords'] = frase_processada

avaliacoes.head()

treinar_modelo(avaliacoes, 'texto_sem_stopwords', 'polarity')

grafico(avaliacoes, 'texto_sem_stopwords', 10)

# removendo pontuacoes e acentuacao
from nltk import tokenize

frase = 'Muito bom, este produto.'
token_pontuacao = tokenize.WordPunctTokenizer()
token_frase = token_pontuacao.tokenize(frase)

token_frase

from string import punctuation
punctuation

pontuacao = list()
for ponto in punctuation:
  pontuacao.append(ponto)
pontuacao

# concatenando a lista pontuacao com palavras_irrelevantes
pontuacao_stopwords = pontuacao + palavras_irrelevantes

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords:
  nova_frase = list()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in pontuacao_stopwords:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao'] = frase_processada

avaliacoes.head()

avaliacoes.texto_sem_stopwords[5]

avaliacoes.texto_sem_stopwords_e_pontuacao[5]

grafico(avaliacoes, "texto_sem_stopwords_e_pontuacao", 10)

!pip install unidecode

import unidecode

acentos = "ótimo péssimo não tão é"

teste = unidecode.unidecode(acentos)
teste

sem_acentos = [unidecode.unidecode(texto) for texto in avaliacoes.texto_sem_stopwords_e_pontuacao]

sem_acentos[4]

avaliacoes.texto_sem_stopwords_e_pontuacao[5]

stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]
stopwords_sem_acento

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos'] = sem_acentos

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:
  nova_frase = list()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos'] = frase_processada

avaliacoes.head(10)

treinar_modelo(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos', 'polarity')

word_cloud_neg(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos')

word_cloud_pos(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos')

frase = 'O Rato Roeu a Roupa do Rei de Roma'
print(frase.lower())

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:
  nova_frase = list()
  avaliacao = avaliacao.lower()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos_minusculo'] = frase_processada

avaliacoes.head()

grafico(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo', 10)

word_cloud_neg(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo')

word_cloud_pos(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo')

treinar_modelo(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo', 'polarity')

grafico(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo', 10)

"""# Stemming e RSLP

(chatgpt)<br/>
A sigla "rslp" se refere a um algoritmo de stemming (redução de palavras à sua forma raiz) específico para o português. O algoritmo "RSLP" significa "Remoção de Sufixos de Lingua Portuguesa" e é projetado para trabalhar com textos em português.

O RSLP é uma das opções de stemming disponíveis na biblioteca NLTK (Natural Language Toolkit) para o idioma português. O stemming é uma técnica de processamento de linguagem natural que reduz as palavras às suas formas básicas (ou raízes), removendo sufixos e prefixos que não são necessários para o significado central da palavra.

Por exemplo, o RSLP pode transformar palavras como "cachorros" ou "cachorrinhos" em sua forma base "cachorro". Ao fazer isso, o algoritmo pode ajudar a simplificar a análise de textos em português e facilitar tarefas como busca de informações ou classificação de textos.
"""

stemmer = nltk.RSLPStemmer()
stemmer.stem("Corredor")
stemmer.stem("Corre")
stemmer.stem("Correria")

avaliacoes.head()

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos_minusculo:
  nova_frase = list()
  avaliacao = avaliacao.lower()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(stemmer.stem(palavra))
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_stemmizado'] = frase_processada

avaliacoes.head()

treinar_modelo(avaliacoes, "texto_sem_stopwords_e_pontuacao_e_acentos_minusculo", "polarity")

treinar_modelo(avaliacoes, "texto_stemmizado", "polarity")

word_cloud_neg(avaliacoes, "texto_stemmizado")

word_cloud_pos(avaliacoes, "texto_stemmizado")

grafico(avaliacoes, "texto_stemmizado", 20)

grafico(avaliacoes, "texto_sem_stopwords_e_pontuacao_e_acentos_minusculo", 20)

"""# DF IDF

(chatgpt)<br/>
TF-IDF (Term Frequency-Inverse Document Frequency) é uma técnica amplamente utilizada em processamento de linguagem natural e recuperação de informações para avaliar a importância de uma palavra em um documento em relação a uma coleção de documentos (corpus). É uma métrica que ajuda a identificar palavras mais relevantes ou importantes para um documento específico.

A técnica TF-IDF é composta por duas partes:

1. Term Frequency (TF): A frequência do termo. É a quantidade de vezes que uma palavra específica aparece em um documento. Normalmente, é calculada dividindo o número de vezes que a palavra aparece pelo número total de palavras no documento.
2. Inverse Document Frequency (IDF): A frequência inversa do documento. É uma medida da raridade de uma palavra em um corpus de documentos. É calculada como o logaritmo do total de documentos divididos pelo número de documentos que contêm a palavra. Se uma palavra é comum em muitos documentos, seu IDF será baixo, indicando que é menos significativa para diferenciar um documento de outro.

O cálculo do TF-IDF para uma palavra em um documento é feito multiplicando o TF da palavra pelo IDF da palavra. Assim, a métrica TF-IDF dá uma maior pontuação a palavras que aparecem frequentemente em um documento, mas raramente em outros documentos, destacando-as como palavras importantes ou discriminativas para o documento.

A fórmula do TF-IDF é:

TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)

Onde:

TF(t,d) é a frequência do termo
<I>t</I> no documento <I>d</I>.

TF-IDF é comumente usado em tarefas como classificação de texto, busca de informações, análise de sentimentos e outras aplicações que envolvem trabalhar com dados textuais. Ele ajuda a identificar palavras-chave ou termos relevantes em um documento e é uma técnica eficaz para pré-processamento de dados textuais antes de alimentar modelos de aprendizado de máquina.

"""

from sklearn.feature_extraction.text import TfidfVectorizer

frases = ['Este produto é muito bom', 'Este produto é muito ruim']
tfidf = TfidfVectorizer(lowercase=False, max_features=100)
caracteristicas = tfidf.fit_transform(frases)

pd.DataFrame(caracteristicas.todense(), columns=tfidf.get_feature_names_out())

avaliacoes.head()

tfidf_tratados = tfidf.fit_transform(avaliacoes.texto_stemmizado)

treino, teste, classe_treino, classe_teste = train_test_split(tfidf_tratados, avaliacoes.polarity, stratify=avaliacoes.polarity, random_state=71)

regressao_logistica = LogisticRegression()
regressao_logistica.fit(treino, classe_treino)
acuraria_tfidf = regressao_logistica.score(teste, classe_teste)
acuraria_tfidf

"""# NGrams

Os n-grams são uma técnica usada em processamento de linguagem natural para dividir um texto em sequências de palavras ou caracteres. Um n-gram é uma sequência de n elementos consecutivos, onde n pode ser qualquer número inteiro maior que zero. Os n-grams podem ser aplicados a palavras ou caracteres, dependendo da granularidade desejada.

Aqui estão os principais tipos de n-grams:

*  Unigramas (1-grams): São palavras ou caracteres isolados. Por exemplo, no texto "O rato roeu a roupa do rei", os unigrams de palavras são ["O", "rato", "roeu", "a", "roupa", "do", "rei"].
*  Bigramas (2-grams): São pares de palavras ou caracteres consecutivos. Por exemplo, no texto "O rato roeu a roupa do rei", os bigrams de palavras são [("O", "rato"), ("rato", "roeu"), ("roeu", "a"), ("a", "roupa"), ("roupa", "do"), ("do", "rei")].
*  Trigramas (3-grams): São sequências de três palavras ou caracteres consecutivos. Por exemplo, no texto "O rato roeu a roupa do rei", os trigrams de palavras são [("O", "rato", "roeu"), ("rato", "roeu", "a"), ("roeu", "a", "roupa"), ("a", "roupa", "do"), ("roupa", "do", "rei")].
*  N-grams: Em geral, n-grams podem ser de qualquer tamanho, desde unigrams até sequências mais longas, como quadgrams (4-grams), pentagrams (5-grams), etc.

Os n-grams são úteis em várias tarefas de processamento de linguagem natural, como:

* Modelagem de linguagem: Os n-grams são usados para calcular a probabilidade de sequências de palavras ou caracteres em um texto. Isso é útil em modelos de linguagem, como aqueles usados para prever palavras em uma sequência de texto.
* Classificação de texto: Os n-grams podem ser usados como recursos para classificar textos com base em padrões frequentes de palavras ou frases.
* Análise de sentimentos: Os n-grams podem ser usados para identificar padrões ou expressões específicas que indicam sentimentos ou opiniões em um texto.

Os n-grams são uma ferramenta importante em processamento de linguagem natural e podem ser usados em conjunto com outras técnicas, como TF-IDF e aprendizado de máquina, para melhorar o desempenho de várias aplicações de processamento de linguagem natural.
"""

from nltk import ngrams

frase = "Comprei um ótimo produto"
frase_separada = token_por_espaco.tokenize(frase)
pares = ngrams(frase_separada, 2) # 2 -> bigram
list(pares)

# sem o bigrams (ngrams)

tfidf = TfidfVectorizer(lowercase=False)
vetor_tfidf = tfidf.fit_transform(avaliacoes.texto_stemmizado)

treino, teste, classe_treino, classe_teste = train_test_split(vetor_tfidf, avaliacoes.polarity, random_state=71)
regressao_logistica = LogisticRegression(max_iter=200)
regressao_logistica.fit(treino, classe_treino)
acuracia_tfidf_ngrams = regressao_logistica.score(teste, classe_teste)
acuracia_tfidf_ngrams

# ngram_range=(1, 2) -> ira gerar quatro vetores para cada palavra (Comprei, um, ótimo, produto)
# e três vetores de pares (('Comprei', 'um'), ('um', 'ótimo'), ('ótimo', 'produto'))
tfidf = TfidfVectorizer(lowercase=False, ngram_range=(1, 2))
vetor_tfidf = tfidf.fit_transform(avaliacoes.texto_stemmizado)

treino, teste, classe_treino, classe_teste = train_test_split(vetor_tfidf, avaliacoes.polarity, random_state=71)
regressao_logistica = LogisticRegression(max_iter=200)
regressao_logistica.fit(treino, classe_treino)
acuracia_tfidf_ngrams = regressao_logistica.score(teste, classe_teste)
acuracia_tfidf_ngrams

# checando os pesos da palavras
pesos = pd.DataFrame(
    regressao_logistica.coef_[0].T,
    index = tfidf.get_feature_names_out()
)
pesos.nlargest(10, 0)

# pesos negativos
pesos.nsmallest(10, 0)

# checando os bigrams positivas
pesos = pd.DataFrame(
    regressao_logistica.coef_[0].T,
    index = tfidf.get_feature_names_out()
)
pesos.nlargest(100, 0)

# checando os bigrams negativas
pesos.nsmallest(100, 0)