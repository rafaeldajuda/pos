# -*- coding: utf-8 -*-
"""fase1_pln_aula2_nltk_e_remocao_stop_words.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16xuAjPJ1D8OYWbqsR6dFKIRlUklcDWa5

# Criando um modelo classificar e avaliação

Utilizando uma base de dados do busca de pé iremos criar um modelo para classificar e avaliar os comentários.

## Importando a base de dados

https://www.kaggle.com/code/abnerfreitas/nlp-buscape-data-ptbr-sentiment-analysis/input
"""

import pandas as pd

avaliacoes = pd.read_csv('b2w.csv')

avaliacoes.head()

"""## Limpeza da base de dados

Retirando as colunas que não são necessárias.
"""

avaliacoes = avaliacoes.drop(['original_index', 'review_text_processed', 'review_text_tokenized', 'rating', 'kfold_polarity', 'kfold_rating'], axis=1)

avaliacoes.head()

"""Remevendo as colunas com valores nulos."""

avaliacoes.info()

avaliacoes.isnull().all().count().sum()

avaliacoes.dropna(inplace=True, axis=0)
avaliacoes

# avaliacoes.polarity.value_counts()
avaliacoes['polarity'].value_counts()

"""## Separando os dados de treino e teste"""

from sklearn.model_selection import train_test_split

treino, teste, classe_treino, classe_teste = train_test_split(avaliacoes.review_text, avaliacoes.polarity,
                                                              stratify=avaliacoes.polarity,
                                                              random_state=71)

classe_teste

"""## Criando um modelo"""

# Linear Model - Logistic Regression
# from sklearn.linear_model import LogisticRegression

# regressao_ligistica = LogisticRegression()
# regressao_ligistica.fit(treino, classe_treino)
# acuracia = regressao_ligistica.score(teste, classe_teste)
# print(acuracia)

# bag of words
from sklearn.feature_extraction.text import CountVectorizer

texto = ['Este produto é muito bom', 'Este produto é muito ruim']
vetorizar = CountVectorizer()
bag_of_words = vetorizar.fit_transform(texto)

bag_of_words

matriz_esparsa = pd.DataFrame.sparse.from_spmatrix(bag_of_words, columns=vetorizar.get_feature_names_out())

matriz_esparsa

"""Vamos aplicar o bag of words na nossa base"""

vetorizar = CountVectorizer()
bag_of_words = vetorizar.fit_transform(avaliacoes.review_text)
print(bag_of_words.shape)

"""116058 -> quatidade de linhas<br/>
50508 -> quantidade de palavras únicas
"""

vetorizar = CountVectorizer(max_features=100) # max_features -> limitando para as 100 palavras que mais se repetem
bag_of_words = vetorizar.fit_transform(avaliacoes.review_text)
print(bag_of_words.shape)

from sklearn.linear_model import LogisticRegression

treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                              avaliacoes.polarity,
                                                              stratify=avaliacoes.polarity,
                                                              random_state=71)

regressao_logistica = LogisticRegression()
regressao_logistica.fit(treino, classe_treino)
acuracia = regressao_logistica.score(teste, classe_teste)
print(acuracia)

from sklearn.linear_model import LogisticRegression

def treinar_modelo(dados, coluna_texto, coluna_sentimento):
  vetorizar = CountVectorizer(max_features=100) # max_features -> limitando para as 100 palavras que mais se repetem
  bag_of_words = vetorizar.fit_transform(avaliacoes[coluna_texto])

  treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                              avaliacoes[coluna_sentimento],
                                                              stratify=avaliacoes[coluna_sentimento],
                                                              random_state=71)

  regressao_logistica = LogisticRegression()
  regressao_logistica.fit(treino, classe_treino)
  return regressao_logistica.score(teste, classe_teste)

print(treinar_modelo(avaliacoes, 'review_text', 'polarity'))

"""# World Cloud

Visualizando as principais reviews com uma word cloud.

https://github.com/amueller/word_cloud
"""

from wordcloud import WordCloud

todas_avaliacoes = [texto for texto in avaliacoes.review_text]
todas_palavras = ' '.join(todas_avaliacoes)

len(todas_avaliacoes)

len(todas_palavras)

nuvem_palavras = WordCloud().generate(todas_palavras)

nuvem_palavras

import matplotlib.pyplot as plt

plt.figure()
plt.imshow(nuvem_palavras)

nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110).generate(todas_palavras)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 7))
plt.imshow(nuvem_palavras, interpolation='bilinear')
plt.axis('off')
plt.show()

"""## Avaliações negativas e positivas"""

def word_cloud_neg(dados, coluna_texto):
  texto_negativo = dados.query('polarity == 0')
  todas_avaliacoes = [texto for texto in texto_negativo[coluna_texto]]
  todas_palavras = ' '.join(todas_avaliacoes)
  # collocations -> retira as palvras em conjunto (ex: muito bom) e deixa somente palavras únicas
  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10, 7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis('off')
  plt.show()

def word_cloud_pos(dados, coluna_texto):
  texto_positivo = dados.query('polarity == 1')
  todas_avaliacoes = [texto for texto in texto_positivo[coluna_texto]]
  todas_palavras = ' '.join(todas_avaliacoes)
  # collocations -> retira as palvras em conjunto (ex: muito bom) e deixa somente palavras únicas
  nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110, collocations=False).generate(todas_palavras)

  plt.figure(figsize=(10, 7))
  plt.imshow(nuvem_palavras, interpolation='bilinear')
  plt.axis('off')
  plt.show()

word_cloud_neg(avaliacoes, 'review_text')

word_cloud_pos(avaliacoes, 'review_text')

"""OBS: nos gráficos existem as **stop words**, que são palavras que não fazem muito sentido na avalição do gráfico. Ex: na, dos, ao, já, ...

# NLTK

(chatgpt)<br/>
A biblioteca NLTK (Natural Language Toolkit) é uma das bibliotecas mais populares em Python para processamento de linguagem natural. Ela oferece uma variedade de ferramentas para trabalhar com textos em várias línguas, com aplicações em áreas como análise de sentimentos, extração de informações e análise semântica.

Recursos principais da NLTK:

* Tokenização: Dividir um texto em unidades linguísticas, como palavras ou sentenças.
* Stemming e lematização: Reduzir palavras à sua forma base (radical) ou forma léxica (lemma).
* Remoção de stopwords: Remover palavras comuns (como artigos e preposições) que não contribuem para o significado do texto.
* Part-of-Speech Tagging: Atribuir etiquetas gramaticais a cada palavra de um texto.
* Chunking: Identificar frases ou trechos de texto com base em suas partes do discurso.
* Classificação e análise de sentimentos: Ferramentas para classificar textos ou analisar sentimentos expressos em textos.
* Corpora e léxicos: Conjuntos de textos anotados e dicionários para ajudar nas tarefas de processamento de linguagem natural.

A NLTK é frequentemente usada em conjunto com outras bibliotecas de aprendizado de máquina para construir modelos mais complexos. É uma biblioteca de grande importância para pesquisadores e profissionais que trabalham com textos e análise de dados textuais.
"""

# https://www.nltk.org/

import nltk
nltk.download("all") # baixa todos pacotes de dados do nltk

# Contando a frequência das frases

corpus = ["Muito este produto", "Muito ruim este produto"]
frequencia = nltk.FreqDist(corpus)
frequencia

from nltk import tokenize

frase = "Muito bom este produto"

token_por_espaco = tokenize.WhitespaceTokenizer()
token_frase = token_por_espaco.tokenize(frase) # ira gerar um array separando as strings pelo espaço
token_frase

token_por_espaco = tokenize.WhitespaceTokenizer()
token_dataset = token_por_espaco.tokenize(todas_palavras)
frequencia = nltk.FreqDist(token_dataset)

frequencia

dataframe_frequencia = pd.DataFrame({'palavra': list(frequencia.keys()), 'frequencia': list(frequencia.values())})

dataframe_frequencia.head()

# listar as palavras mais frequêntes
dataframe_frequencia.nlargest(columns='frequencia', n=10)

import seaborn as sns

plt.figure(figsize=(12, 8))
ax = sns.barplot(data = dataframe_frequencia.nlargest(columns='frequencia', n=10), x='palavra', y='frequencia', color='lightblue')
ax.set(ylabel='Contagem')
plt.show()

def grafico(dados, coluna_texto, quantidade):
  todas_palavras = ' '.join(texto for texto in dados[coluna_texto])
  token_frase = token_por_espaco.tokenize(todas_palavras)
  frequencia = nltk.FreqDist(token_frase)
  dataframe_frequencia = pd.DataFrame({'palavra': list(frequencia.keys()), 'frequencia': list(frequencia.values())})
  dataframe_frequencia = dataframe_frequencia.nlargest(columns='frequencia', n=quantidade)

  plt.figure(figsize=(12, 8))
  ax = sns.barplot(data = dataframe_frequencia, x='palavra', y='frequencia', color='lightblue')
  ax.set(ylabel='Contagem')
  plt.show()

grafico(avaliacoes, 'review_text', 20)

"""# Remoção de Stop Words"""

palavras_irrelevantes = nltk.corpus.stopwords.words('portuguese')
palavras_irrelevantes

frase_processada = list()
for avaliacao in avaliacoes.review_text:
  nova_frase = list()
  palavras_texto  = token_por_espaco.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in palavras_irrelevantes:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords'] = frase_processada

avaliacoes.head()

treinar_modelo(avaliacoes, 'texto_sem_stopwords', 'polarity')

grafico(avaliacoes, 'texto_sem_stopwords', 10)

# removendo pontuacoes e acentuacao
from nltk import tokenize

frase = 'Muito bom, este produto.'
token_pontuacao = tokenize.WordPunctTokenizer()
token_frase = token_pontuacao.tokenize(frase)

token_frase

from string import punctuation
punctuation

pontuacao = list()
for ponto in punctuation:
  pontuacao.append(ponto)
pontuacao

# concatenando a lista pontuacao com palavras_irrelevantes
pontuacao_stopwords = pontuacao + palavras_irrelevantes

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords:
  nova_frase = list()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in pontuacao_stopwords:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao'] = frase_processada

avaliacoes.head()

avaliacoes.texto_sem_stopwords[5]

avaliacoes.texto_sem_stopwords_e_pontuacao[5]

grafico(avaliacoes, "texto_sem_stopwords_e_pontuacao", 10)

!pip install unidecode

import unidecode

acentos = "ótimo péssimo não tão é"

teste = unidecode.unidecode(acentos)
teste

sem_acentos = [unidecode.unidecode(texto) for texto in avaliacoes.texto_sem_stopwords_e_pontuacao]

sem_acentos[4]

avaliacoes.texto_sem_stopwords_e_pontuacao[5]

stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]
stopwords_sem_acento

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos'] = sem_acentos

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:
  nova_frase = list()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos'] = frase_processada

avaliacoes.head(10)

treinar_modelo(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos', 'polarity')

word_cloud_neg(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos')

word_cloud_pos(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos')

frase = 'O Rato Roeu a Roupa do Rei de Roma'
print(frase.lower())

frase_processada = list()
for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:
  nova_frase = list()
  avaliacao = avaliacao.lower()
  palavras_texto  = token_pontuacao.tokenize(avaliacao)
  for palavra in palavras_texto:
    if palavra not in stopwords_sem_acento:
      nova_frase.append(palavra)
  frase_processada.append(' '.join(nova_frase))

avaliacoes['texto_sem_stopwords_e_pontuacao_e_acentos_minusculo'] = frase_processada

avaliacoes.head()

grafico(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo', 10)

word_cloud_neg(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo')

word_cloud_pos(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo')

treinar_modelo(avaliacoes, 'texto_sem_stopwords_e_pontuacao_e_acentos_minusculo', 'polarity')