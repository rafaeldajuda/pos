# -*- coding: utf-8 -*-
"""fase3_ia_aula2_3_rede_neural_multilayer_perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzqo4jbDWHDA3fhGuu1DMCGiNpapq9DO

# Classificando diferentes tipos de semente de ab√≥bora üéÉüå±

As sementes de ab√≥bora s√£o frequentemente consumidas como confeitos em todo o mundo devido √† sua quantidade adequada de prote√≠nas, gorduras, carboidratos e teores minerais. A base de dados "SementesAbobora.xlsx" possui um estudo foi realizado nos dois tipos de sementes de ab√≥bora mais importantes e de qualidade, ‚Äú√úrg√ºp Sivrisi‚Äù e ‚Äú√áer√ßevelik‚Äù, geralmente cultivadas nas regi√µes de √úrg√ºp e Karaca√∂ren na Turquia.

Muitas esp√©cies de sementes t√™m semelhan√ßas visuais, o que torna a classifica√ß√£o manual dif√≠cil e sujeita a erros. Redes neurais podem ser treinadas para identificar padr√µes que n√£o s√£o facilmente percept√≠veis pelo olho humano, aumentando a precis√£o da classifica√ß√£o.

Imagine que foi proposto para voc√™ o desafio de criar uma intelig√™ncia para identificar os tipos de sementes para ajudar a equipe de engenheiros e engenheiras Agr√≠colas. Para trabalhar com a precis√£o dos resultados x complexidade das caracter√≠sticas de sementes, voc√™ optou em utilizar as redes neurais multilayer perceptron. Vamos para a aplica√ß√£o?

## Features

* Per√≠metro
* Maior_Eixo_Comprimento
* Comprimento_Eixo_Menor
* √Årea_Convexa
* Equiv_Di√¢metro
* Excentricidade
* Solidez
* Extens√£o
* Redondeza
* Proporcao
* Compacidade

## Target

Classes: ((A)√áer√ßevelik, (B)√úrg√ºp Sivrisi)

Vamos colocar a m√£o na massa. Vamos importar a base de dados "SementesAbobora.xlsx" utilizando a biblioteca pandas. E analisar a dimens√£o dos dados.
"""

import pandas as pd

df = pd.read_excel('SementesAbobora.xlsx')

df.head()

df.tail()

df.shape

"""Observe que essa base de dados possui os dados de classes de forma ordenada, isso pode ser um problema para o aprendizado de m√°quina. Vamos come√ßar e j√° embaralhar com o comando shuffle."""

from sklearn.utils import shuffle

df = shuffle(df)

df.head()

"""Muito bem, agora temos os dados embaralhados. Pr√≥ximo passo, vamos conhecer o equil√≠brio das nossas classes (j√° que temos um problema de classica√ß√£o a ser resolvido)."""

df['Class'].value_counts()

df['Class'].value_counts(normalize=True)

"""Base est√° equilibrada, √≥timo! Caso n√£o estivesse, ter√≠amos que aplicar t√©cnicas de reamostragem de dados ou at√© mesmo coletar mais dados para a cria√ß√£o de nossa rede neural.

Vamos analisar os dados com uma an√°lise explorat√≥ria?
"""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Selecionar apenas colunas num√©ricas
numeric_df = df.select_dtypes(include=[np.number])

# Calcular a matriz de correla√ß√£o
correlation_matrix = numeric_df.corr().round(2)
fig, ax = plt.subplots(figsize=(15,10))
sns.heatmap(data=correlation_matrix, annot=True, linewidths=.5, ax=ax)

"""A correla√ß√£o √© muito importante para entendermos as rela√ß√µes das vari√°veis (ou seja, a associa√ß√£o entre duas vari√°veis). Podemos identificar que temos vari√°veis altamente correlacionadas e sabemos que isso pode ser um problema para o modelo.

Area, Area_convexa, Equiv_Di√¢metro e Maior_Eixo_Comprimento possuem correla√ß√£o mair que **0.90** quando analisadas com a vari√°vel **Per√≠metro**, isso se deve pelo motivo que todas essas vari√°veis s√£o relacionadas ao tamanho das sementes. J√° sabemos que n√£o precisamos colocar todas no modelo.

## Tratando a vari√°vel target
"""

# Utilizando Label Enconder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df.Class = le.fit_transform(df['Class'])

set(df['Class'])

"""## Separando os dados

Como todo aprendizado de m√°quina, precisamos separar as bases de treino e teste! Vamos incluir todas as vari√°veis do modelo exceto as que est√£o altamente correlacionadas entre si (Area, Area_convexa, Equiv_Di√¢metro e Maior_Eixo_Comprimento). Dentre as vari√°veis como muita correla√ß√£o, vamos selecionar apenas a Area.
"""

X = df[['Area', 'Per√≠metro', 'Comprimento_Eixo_Menor', 'Excentricidade', 'Solidez', 'Extens√£o', 'Redondeza', 'Proporcao', 'Compacidade']]
y = df['Class']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

"""## Pr√©-processamento dos dados: normalizando as escalas

Apesar de deep learning n√£o requerer distribui√ß√µes normais, √© comum aplicar **t√©cnicas de pr√©-processamento de dados para normalizar as caracter√≠sticas**. Isso pode ajudar a acelerar a converg√™ncia do treinamento da rede neural.

A converg√™ncia est√° relacionada com o erro, ou seja, o quanto a sua rede aprende a corrigir os erros durante o processamento.
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

X_train

"""## Moldando nossas vari√°veis resposta

Perceba que a nossa classes est√£o em um formato de estrutura de dados unidimensional (assim como uma coluna) e precisamos moldar para o formato de array n dimensional (matriz). Os famosos tensores!

O comando reshape √© utilizado para reformatar a estrutura de um array multidimensioanal, como um tensor, que √© estrutura de dados fundamental usada em deep learning e processamento de dados em redes neurais.
"""

y_train.shape

import numpy as np

# reshape() molda uma matriz sem alterar os dados da matriz.
y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))

y_train.shape

type(y_train)

"""## Construindo a arquitetura da rede neural multicamadas

Agora utilizando as bibliotecas tensorflow e keras, vamos contruir a arquitetura da nossa primeira rede neural.
"""

import tensorflow as tf

# keras libraries
from tensorflow import keras
from keras import models
from keras import layers
from keras import metrics
from keras.optimizers import Adam
from keras.metrics import Precision
from tqdm.keras import TqdmCallback

X_train.shape[1]  # total de colunas

y_train.shape[1]  # total de colunas

# Semente aleat√≥ria para manter os mesmos dados
tf.random.set_seed(7)

# Definindo entradas da rede + tamanho de batch de processamento
input_shape = X_train.shape[1]  # Vari√°veis de entrada
output_shape = y_train.shape[1] # Classe preditora
batch_size = 100

# Abrindo uma sequencia de neuronios
model = models.Sequential()

# input layer
# Entrada de rede
model.add(layers.Dense(
    batch_size,
    input_shape=(input_shape,),
    activation='relu'
))

# hidden layer
# Camada oculta
model.add(layers.Dense(
    24,
    activation='relu'
))

# hidden layer
# Camada oculta
model.add(layers.Dense(
    18,
    activation='relu'
))

# dropout layer
# Aplicando regulariza√ß√£o
model.add(layers.Dropout(0.2))

# output layer
# Camada de sa√≠da
model.add(layers.Dense(
    output_shape,
    activation='sigmoid'
))

# Configurar o otimizador Adam com uma learning rate espec√≠fica
# Defina a learning rate desajada
learning_rate = 0.001
otimizador = Adam(learning_rate=learning_rate)

# Compilar o modelo com o otimizador configurado
model.compile(loss='binary_crossentropy', optimizer=otimizador, metrics=['accuracy'])

# summary
model.summary()

"""Agora vamos executar as √©pocas de processamento para a rede treinar e encontrar o menor erro:"""

# Configurando as √©pocas de processamento para a converg√™ncia do erro da fun√ß√£o de custo
epoch = 80

hist = model.fit(X_train,
                 y_train,
                 epochs=epoch,
                 batch_size=batch_size,
                 shuffle=True,
                 validation_data=(X_test, y_test),
                 verbose=0,
                 callbacks=[TqdmCallback(verbose=0)]) # 0 sem log - 1 ver log

acc = '{:.2%}'.format(hist.history['accuracy'][-1])
print(f'O modelo possui uma acur√°cia de {acc} com {epoch} epochs de processamento')

"""## Validando nosso modelo

√â muito importante comparar a performance do modelo tanto na base de treinamento quanto de valida√ß√£o. Para isso vamos plotar dois gr√°ficos para acompanhar a performace do modelo pelas √©pocas de processamento.
"""

# Visualizando os resultados de treino
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

loss = hist.history['loss']
val_loss = hist.history['val_loss']

epochs_range = range(epoch)

# Plot Acur√°cia
plt.figure(figsize=(20,8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Acur√°cia de Treinamento')
plt.plot(epochs_range, val_acc, label='Acur√°cia de Valida√ß√£o')
plt.legend(loc='lower right')
plt.title('Acur√°cia de treino e teste')

# Plot Erro de treinamento
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Erro de treinamento')
plt.plot(epochs_range, val_loss, label='Erro de valida√ß√£o')
plt.legend(loc='upper right')
plt.title('Erro de treinamento vs valida√ß√£o')
plt.show()

from sklearn.metrics import classification_report

# Predictions
y_pred = model.predict(X_test)
y_pred_class = [round(x[0]) for x in y_pred]
y_test_class = y_test

# classification report
class_names = []
for i in y.unique():
  class_names.append(le.inverse_transform([i])[0])

print(classification_report(y_test_class, y_pred_class, target_names=class_names))