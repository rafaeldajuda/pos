# -*- coding: utf-8 -*-
"""fase3_deep_e_reinforcement_learning_aula1_Multilayer_Perceptron_Classificando_sementes_de_ab√≥bora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ftDyBYkF0S5Cbl3-cTckLazriYfi341k

# Classificando diferentes tipos de semente de ab√≥bora üéÉüå±

As sementes de ab√≥bora s√£o frequentemente consumidas como confeitos em todo o mundo devido √† sua quantidade adequada de prote√≠nas, gorduras, carboidratos e teores minerais. A base de dados **"SementesAbobora.xlsx"** possui um estudo foi realizado nos dois tipos de sementes de ab√≥bora mais importantes e de qualidade, **‚Äú√úrg√ºp Sivrisi‚Äù e ‚Äú√áer√ßevelik‚Äù**, geralmente cultivadas nas regi√µes de √úrg√ºp e Karaca√∂ren na Turquia.

Muitas esp√©cies de sementes t√™m semelhan√ßas visuais, o que torna a classifica√ß√£o manual dif√≠cil e sujeita a erros. Redes neurais podem ser treinadas para identificar padr√µes que n√£o s√£o facilmente percept√≠veis pelo olho humano, aumentando a precis√£o da classifica√ß√£o.

Imagine que foi proposto para voc√™ o desafio de criar uma **intelig√™ncia para identificar os tipos de sementes para ajudar a equipe de engenheiros e engenheiras Agr√≠colas**. Para trabalhar com a precis√£o dos resultados x complexidade das caracter√≠sticas de sementes, voc√™ optou em utilizar as redes neurais multilayer perceptron. Vamos para a aplica√ß√£o?

## Features

*  Per√≠metro
*  Maior_Eixo_Comprimento
*  Comprimento_Eixo_Menor
*  √Årea_Convexa
*  Equiv_Di√¢metro
*  Excentricidade
*  Solidez
*  Extens√£o
*  Redondeza
*  Proporcao
*  Compacidade

## Target

Classes: ((A)√áer√ßevelik, (B)√úrg√ºp Sivrisi)

Vamos colocar a m√£o na massa. Vamos importar a base de dados "SementesAbobora.xlsx" utilizando a biblioteca pandas. E analisar a dimens√£o dos dados.
"""

import pandas as pd

df = pd.read_excel('SementesAbobora.xlsx')

df.head()

df.shape

"""Observe que essa base de dados possui os dados de classes de forma ordenada, isso pode ser um problema para o aprendizado de m√°quina. Vamos come√ßar a j√° embaralhar os dados com o comando shuffle."""

from sklearn.utils import shuffle

df = shuffle(df)

df.head(10)

"""Muito bem, agora temos os dados embaralhados. Pr√≥ximo passo, vamos conhecer o equil√≠brio das nossas classes (j√° que temos um problema de classifica√ß√£o a ser resolvido)."""

df['Class'].value_counts()

"""Base est√° equilibrada, √≥timo! Caso n√£o estivesse, ter√≠amos que aplicar t√©cnicas de reamostragem de dados ou at√© mesmo coletar mais dados para a cria√ß√£o de nossa rede neural.

Vamos analisar os dados com uma an√°lise explorat√≥ria? üìä
"""

# Visualiza√ß√£o
import seaborn as sns
import matplotlib.pyplot as plt

# Potando histogramas para analisar a simetria dos dados
df.hist(bins=100, figsize=(12, 12))
plt.show()

"""Podemos observar que existem muitas vari√°veis com a distribui√ß√£o quase que normal, ou seja, n√£o temos muitos outliers exeto para as vari√°veis: Proporcao, Redondeza e Compacidade que possuem distribui√ß√£o assim√©trica muito forte.

Que tal analisarmos as correla√ß√µes?
"""

correlation_matrix = df.corr(numeric_only=True).round(2)

fig, ax = plt.subplots(figsize=(15,10))
sns.heatmap(data=correlation_matrix, annot=True, linewidths=.5, ax=ax)

"""A correla√ß√£o √© muito importante para entendermos as rela√ß√µes das vari√°veis (ou seja, a associa√ß√£o entre duas vari√°veis). Podemos identificar aqui que temos vari√°veis altamente correlacionadas e sabemos que isso pode ser um problema para o modelo.

Area, Area_convexa, Equiv_Di√¢metro e Maior_Eixo_Comprimento possuem correla√ß√£o maior que **0.90** quando analisadas com a vari√°vel **Per√≠metro**, isso se deve pelo motivo que que todas essas vari√°veis s√£o relacionadas ao tamanho das sementes. J√° sabemos que n√£o precisamos colocar todas no modelo.

Vamos passar um pouco r√°pido pela etapa de an√°lise explorat√≥ria pois n√£o √© o foco dessa aula, mas eu sempre gosto de relembrar a import√¢ncia de uma boa an√°lise explorat√≥ria nos dados.

# Tratando a vari√°vel target
"""

df.info()

# Utilizadno Label Enconder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df.Class = le.fit_transform(df['Class'])

set(df['Class'])

"""## Separando os dados

Como todo aprendizado de m√°quina, precisamos separar as bases de treino e teste! Vamos incluir todas as vari√°veis do modelo exceto as que est√£o altamente correlacionadas entre si (Area, Area_convexa, Equiv_Di√¢metro e Maior_Eixo_Comprimento). Dentre as vari√°veis com muita correla√ß√£o, vamos selecionar apenas a Area.
"""

df.info()

X = df[['Area','Per√≠metro', 'Comprimento_Eixo_Menor','Excentricidade','Solidez','Extens√£o','Redondeza', 'Proporcao', 'Compacidade']]
y = df['Class']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

"""## Pr√©-processamento dos dados

Apesar de deep learning n√£o requerer distribui√ß√µes normais, √© comum aplicar **t√©cnicas de pr√©-processamento de dados para normalizar ou padronizar as caracter√≠sticas**. Isso pode ajudar a acelerar a converg√™ncia do treinamento da rede neural.

A converg√™ncia est√° relacionada com o erro, ou seja, o quanto a sua rede aprende a corrigir os erros durante o processamento.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
scaler.fit(X_test)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

X_train.shape

X_test.shape

"""## Moldando nossas vari√°veis resposta

Perceba que a nossas classes est√£o em um formato de estrutura de dados unidimensional (assim como uma coluna) e precisamos moldar para o formato de array -n dimensional(matriz).

O comando reshape √© utilizado para reformatar a estrutura de um array multidimensional, como um tensor, que √© a estrutura de dados fundamental usada em deep learning e processamento de dados em redes neurais.
"""

type(y_train)

import numpy as np

# reshape() molda uma matriz sem alterar os dados da matriz.
y_train = np.asarray(y_train).astype('float32').reshape((-1,1))
y_test = np.asarray(y_test).astype('float32').reshape((-1,1))

type(y_train)

y_train.shape

"""## Construindo a arquitetura da rede neural multicamadas ü¶æ

Agora utilizando as bibliotecas tensorflow e keras, vamos construir a arquitetura da nossa primeira rede neural.
"""

!pip install tensorflow

import tensorflow as tf

# keras libraries
from tensorflow import keras
from keras import models
from keras import layers
from keras import metrics
from keras.optimizers import Adam
from keras.metrics import Precision
from tqdm.keras import TqdmCallback

# Semente aleat√≥tia para manter os mesmos dados
tf.random.set_seed(7)

# Definindo entradas da rede + tamanho da batch de processamento
input_shape = X_train.shape[1]    # Vari√°veis de entrada
output_shape = y_train.shape[1]   # Classe preditora
batch_size = 20

# Abrindo uma sequencia de neuronios
model = models.Sequential()

# input layer
# Entrada da rede
model.add(layers.Dense(
                        batch_size
                       ,input_shape=(input_shape,)
                       ,activation='relu'))

# hidden layer
# Camada oculta
model.add(layers.Dense(
                        12
                       ,activation='relu'))

# hidden layer
# Camada oculta
model.add(layers.Dense(
                        6
                       ,activation='relu'))


# dropout layer
# Aplicando regulariza√ß√£o
model.add(layers.Dropout(0.5))

# output layer
# Camada de sa√≠da
model.add(layers.Dense(
                        output_shape
                       ,activation='sigmoid'))

# Configurar o otimizador Adam com uma learning rate espec√≠fica
# Defina a learning rate desejada
learning_rate = 0.001
otimizador = Adam(learning_rate=learning_rate)

# Compilar o modelo com o otimizador configurado
model.compile(loss='binary_crossentropy', optimizer=otimizador, metrics=['accuracy'])

# summmary
model.summary()

"""Agora vamos executar as √©pocas de processamento para a rede treinar e encontrar o menor erro:"""

# Configurando as √©pocas de processamento para a converg√™ncia do erro da fun√ß√£o de custo
epoch = 100

hist = model.fit(X_train
                  ,y_train
                  ,epochs = epoch
                  ,batch_size=batch_size
                  ,shuffle=True
                  ,validation_data=(X_test, y_test)
                  ,verbose=0
                  ,callbacks=[TqdmCallback(verbose=0)]
          )

acc = '{:.2%}'.format(hist.history['accuracy'][-1])
print(f"O modelo possui uma acur√°cia de {acc} com {epoch} epochs de processamento")

"""## Validando nosso modelo

√â muito importante comparar a performance do modelo tanto na base de treinamento quanto de valida√ß√£o. Para isso vamos plotar dois gr√°ficos para acompanhar a performance do modelo pelas √©pocas de processamento.
"""

# Visualizando os resultados de treino
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

loss = hist.history['loss']
val_loss = hist.history['val_loss']

epochs_range = range(epoch)

# Plot Acur√°cia
plt.figure(figsize=(20, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Acur√°cia de Treinamento')
plt.plot(epochs_range, val_acc, label='Acur√°cia de Valida√ß√£o')
plt.legend(loc='lower right')
plt.title('Acur√°cia de treino e teste')

# Plot Erro de treinamento
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Erro de treinamento')
plt.plot(epochs_range, val_loss, label='Erro de Valida√ß√£o')
plt.legend(loc='upper right')
plt.title('Erro de treinamento vs valida√ß√£o')
plt.show()

from sklearn.metrics import classification_report
# Predictions
y_pred = model.predict(X_test)
y_pred_class = [round(x[0]) for x in y_pred]
y_test_class = y_test

# classification report
class_names = []
for i in y.unique():
    class_names.append(le.inverse_transform([i])[0])

print(classification_report(y_test_class, y_pred_class, target_names=class_names))